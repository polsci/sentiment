{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 Lab Class: Sentiment Analysis\n",
    "\n",
    "Make sure you use the Python 3.12 Kernel to run this notebook. \n",
    "\n",
    "This lab will investigate lexicon-based sentiment analysis with VADER (‘Valence Aware Dictionary for sEntiment Reasoning’). VADER is open source software, so you can inspect the code and modify it if you wish. In this week’s lab we will mainly refer to the lexicon.\n",
    "\n",
    "Although VADER is more than 10 years old, it is still commonly used. You can learn lots about how language expresses sentiment by using VADER, understanding how it works, when it works and when it doesn't. \n",
    "\n",
    "The following cells imports libraries and creates a SentimentIntensityAnalyzer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textplumber.report import plot_confusion_matrix, plot_logistic_regression_features_from_pipeline, preview_dataset\n",
    "from textplumber.embeddings import Model2VecEmbedder\n",
    "from textplumber.store import TextFeatureStore\n",
    "\n",
    "# Textplumber implements VADER scoring of texts, and extraction of sentiment features\n",
    "from textplumber.vader import VaderSentimentEstimator, VaderSentimentExtractor, VaderSentimentProfileExtractor, SentimentIntensityInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cmap = LinearSegmentedColormap.from_list(\"red_white_green\", [\"red\", \"white\", \"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_score(s):\n",
    "    score_min = -1\n",
    "    score_max = 1\n",
    "    return (s - score_min) / (score_max - score_min) \n",
    "\n",
    "def highlight_row(row):\n",
    "    normed = norm_score(row['compound'])\n",
    "    color = to_hex(custom_cmap(normed))\n",
    "    return [f'background-color: {color}; color: black'] * len(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "interpreter = SentimentIntensityInterpreter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learn about VADER scores\n",
    "\n",
    "In the cell below is a short phrase to show you the output of VADER. Get VADER's scores for the provided text and make sure you understand what each number tells us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "This movie is terrible.\n",
    "'''\n",
    "vs = analyzer.polarity_scores(example)\n",
    "print(str(vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.explain(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the \"About the Scoring\" section of the Vader Github README, which explains the scores that are returned by Vader:  \n",
    "https://github.com/cjhutto/vaderSentiment#about-the-scoring\n",
    "\n",
    "### 1.1 Questions\n",
    "\n",
    "1. What do the 'neu', 'pos', and 'neg' scores represent?  \n",
    "2. What range of values of the Compound Score should be associated with a \"neutral\" classification?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Score some text and understand Vader's lexicon and booster/negation rules\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example - you can copy and paste this code into new code cells to test out different phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "The movie was great.\n",
    "'''\n",
    "vs = analyzer.polarity_scores(example)\n",
    "print(str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Activities\n",
    "\n",
    "Try different text and make sure you understand the scores VADER returns. Copy the code above into new cells below for each example you come up with.\n",
    "\n",
    "Create examples for the following conditions:   \n",
    "\n",
    "1. A sentence that is obviously positive like \"The movie is great\"\n",
    "2. A sentence that uses a \"booster\" e.g. \"The movie is really terrible\"\n",
    "3. A sentence that uses negation e.g. \"The movie is not great\". \n",
    "4. Some sentences that attempts to fool Vader. Think about the discussion in class and challenges with sentiment classification in general, and specific challenges related to VADER's lexicon or rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aide your understanding of VADER\n",
    "\n",
    "Look at the lexicon and the booster/negation words on the VADER repository to get more insight into the scores.\n",
    "\n",
    "* The VADER module code is here: https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py  \n",
    "* Negations and booster words are on lines 48-181.  \n",
    "* The Vader lexicon is available here: https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt  Note: you can search the lexicon in your browser or you can download it and inspect it in a text editor.  \n",
    "* Make sure you are clear what the values in the VADER lexicon actually mean.  \n",
    "\n",
    "Here are some examples for your reference: \n",
    "\n",
    "    hope \t1.9 0.53852 [3, 2, 2, 1, 2, 2, 1, 2, 2, 2]\n",
    "    hopeless -2.0 1.78885 [-3, -3, -3, -3, 3, -1, -3, -3, -2, -2]\n",
    "\n",
    "* The VADER paper itself is helpful also: https://ojs.aaai.org/index.php/ICWSM/article/view/14550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score longer texts\n",
    "\n",
    "Below we load the movie reviews dataset we used in a previous lab. \n",
    "\n",
    "You can browse the dataset here: https://huggingface.co/datasets/polsci/sentiment-polarity-dataset-v2.0\n",
    "or download the texts here: https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/movie_reviews.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('polsci/sentiment-polarity-dataset-v2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(dataset['train']['text'])\n",
    "y = list(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = dataset['train'].features['label'].names\n",
    "target_classes = list(range(len(target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Activity:\n",
    "\n",
    "Run the cells below to preview a review and get VADER's scores.\n",
    "\n",
    "Try some different reviews from the dataset and see what scores Vader comes up with. Are the scores correct against the actual label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_id = 904\n",
    "try:\n",
    "    review = X_train[review_id]\n",
    "    print(f\"Label: {target_names[y_train[review_id]]}\")\n",
    "    print()\n",
    "    print(review)\n",
    "except IndexError:\n",
    "    print(f\"Review ID {review_id} is out of range for the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = analyzer.polarity_scores(review)\n",
    "print(str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluating VADER's performance on long texts\n",
    "\n",
    "This cells below compare VADER scoring and a model based on classification of embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, here is an evaluation of VADER.\n",
    "\n",
    "Note: The labels in the dataset are either positive or negative (i.e. no neutral). Here compound scores greater than or equal to 0 are considered positive, and scores less than 0 are considered negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('classifier', VaderSentimentEstimator(output = 'labels', neutral_threshold = 0, label_mapping = {'positive': 1, 'negative': 0})),\n",
    "], verbose=True)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, labels = target_classes, target_names = target_names, digits=3))\n",
    "plot_confusion_matrix(y_test, y_pred, target_classes, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, here is a model based on a Logistic Regression classifier and Model2Vec embeddings on the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = TextFeatureStore('sentiment-lab-movie_reviews.sqlite')\n",
    "pipeline = Pipeline([\n",
    "        ('features', Model2VecEmbedder(feature_store = feature_store)),\n",
    "        ('classifier', LogisticRegression(max_iter = 5000, random_state=55))\n",
    "], verbose=True)\n",
    "display(pipeline)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, labels = target_classes, target_names = target_names, digits=3))\n",
    "plot_confusion_matrix(y_test, y_pred, target_classes, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels based on VADER scores are accurate more often than not, but accuracy is not great on these long texts (around 65% on this dataset) when compared to a basic classifier (around 75%). Observe that on these long reviews, VADER has a tendency to label reviews as positive more than negative. \n",
    "\n",
    "VADER works better on short texts. The original VADER paper indicates it worked best on social media texts.\n",
    "\n",
    "Despite these limitations, we can use VADER to explore some of the problems deriving overall sentiment scores using a lexicon-based approach and some of the challenges of measuring sentiment more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examining sentiment scores by sentence\n",
    "\n",
    "Let’s look at an example review to think about the different frames of reference to which sentiments might be connected. The example we will use is a review of Neil Jordan’s film The Butcher Boy filename cv079_11933.txt. \n",
    "\n",
    "A descriptive statement describes the content of the film. Eg sentence 3: Francie is a “sick, needy child” - this tells us about what happens in the film.\n",
    "\n",
    "An analytic statement analyses the content of the film. \n",
    "\n",
    "Eg sentence 3: “I found it difficult to laugh at some of Francie’s darkly comic shenanigans” - here the reviewer is analysing the effects of the film.\n",
    "\n",
    "It’s not a perfect distinction, but we can observe that negative content in the film doesn’t necessarily imply a negative review of the film. Both types of statements can include evaluative language and include indications of the reviewer's point of view about the movie, but lexicon-based sentiment analysis will have difficulty if a review has a lot of “negative” content, but is nonetheless given a positive review.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Activity\n",
    "Run the following cells to split the text into sentences and output scores for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_id = 904 # change the review ID to examine another review\n",
    "try:\n",
    "    review = X_train[review_id]\n",
    "    # this splits the review using NLTK's sentence tokenizer and removes empty sentences or sentences with only common punctuation\n",
    "    sentences = sent_tokenize(review)\n",
    "    sentences = [s for s in sentences if s.strip() and s.strip() not in ['.', ',', '!', '?']]\n",
    "    print(f\"Label: {target_names[y_train[review_id]]}\")\n",
    "    print()\n",
    "    display(sentences)\n",
    "except IndexError:\n",
    "    print(f\"Review ID {review_id} is out of range for the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    data.append([sentence, vs['neg'], vs['neu'], vs['pos'], vs['compound']])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['sentence','neg','neu','pos','compound'])\n",
    "\n",
    "print(f\"Label: {target_names[y_train[review_id]]}\")\n",
    "print()\n",
    "display(df.style.apply(highlight_row, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Questions\n",
    "\n",
    "1. Look closely at each sentence and work out which ones relate to the reviewer's evaluation of the movie. Is Vader doing a good job of scoring these sentences?  \n",
    "2. Try this with another review. Change the ID number in the cell above to load another review. Look carefully at the positively and negatively evaluated sentences using the compound score. From this analysis, what challenges do you see in correctly assigning overall sentiment scores to movie reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examining the structure of reviews\n",
    "\n",
    "In class we talked about the argumentative structure of reviews, what reviewers are doing when they write a review and who a review is for. When evaluating a film, reviewers rarely just say \"Loved it\" or \"Hated it\", that is what the number rating is for. Reviewers tend to craft an argument that justifies their rating, using the descriptive and analytical statements discussed above. Reviewers also tend to follow conventions of other reviews they've read and anticipate that people read reviews to find movies to watch. For a reviewer to be viewed as credible and their review to be useful to its potential audience, reviewers will often point out positive and negative features of a film, while expressing their evaluation. This weighing up of good and bad may help readers understand if a film is suitable for them or not. In a positive review, we can expect some discussion of negative features, and in a negative review we can expect some discussion of positive features. \n",
    "\n",
    "This point is not just about reviews, this is a general point about the structure of opinion-giving. Part of giving your view is anticipating the views of others.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Activity\n",
    "\n",
    "Below is an example of a review that discusses positive and negative features of a film and discusses who the film might be suitable for. Change the ID number and examine how other review authors are orienting to their audience and structuring their evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_id = 214 # change the review ID to examine another review\n",
    "try:\n",
    "    review = X_train[review_id]\n",
    "    # this splits the review using NLTK's sentence tokenizer and removes empty sentences or sentences with only common punctuation\n",
    "    sentences = sent_tokenize(review)\n",
    "    sentences = [s for s in sentences if s.strip() and s.strip() not in ['.', ',', '!', '?']]\n",
    "\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        data.append([sentence, vs['neg'], vs['neu'], vs['pos'], vs['compound']])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['sentence','neg','neu','pos','compound'])\n",
    "\n",
    "    print(f\"Label: {target_names[y_train[review_id]]}\")\n",
    "    print()\n",
    "    display(df.style.apply(highlight_row, axis=1))\n",
    "except IndexError:\n",
    "    print(f\"Review ID {review_id} is out of range for the training set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Looking at structure across the corpus\n",
    "\n",
    "The following visualisation shows some interesting patterns from exploratory analysis of the review corpus. The visualisation clusters reviews by the structure of sentiment scores. Take a look at the visualisation now. There are some notes at the bottom of the image to help you interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VaderSentimentProfileExtractor(output='profileonly').plot_sentiment_structure(X_train, y_train, target_classes = target_classes, target_names = target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Questions about the visualisation\n",
    "\n",
    "1. What are some differences you notice between positive and negative reviews?  \n",
    "2. Are there clusters you would expect to be misclassified by VADER across the whole document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What happens if we classify based on VADER scores across documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell trains a model based on multiple sentiment features for each text. The sentiment profile for each text includes the overall VADER compound score and positive/negative/neutral proportions, as well as sentiment scores across the structure of the texts. Compound scores are extracted for the first three sentences, the last three sentences and four random sentences from the middle of the text. Short texts are handled by padding the extracted features with zeros. \n",
    "\n",
    "This takes a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('features', VaderSentimentProfileExtractor(output='profile')),\n",
    "        ('classifier', LogisticRegression(max_iter = 5000, random_state=55))\n",
    "], verbose=True)\n",
    "display(pipeline)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, labels = target_classes, target_names = target_names, digits=3))\n",
    "plot_confusion_matrix(y_test, y_pred, target_classes, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not reaching the performance of the model based on emebddings, this model outperforms the overall VADER scoring of long texts. These sentiment features can be combined with other features to improve performance further.\n",
    "\n",
    "Take a moment to review the plot of discriminative features below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logistic_regression_features_from_pipeline(pipeline, target_classes, target_names, top_n=20, classifier_step_name = 'classifier', features_step_name = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of discriminative features is interesting, as it shows the model learned the relative importance of sentiment scores for the conclusion of reviews over the introduction or body in predicting a documents sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Concluding Activities and Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **ACTIVITY:** In class this week we discussed how sentiment analysis might not be an appropriate technique for analysing some kinds of texts. For example, some texts are not primarily about presenting a point of view or evaluation (e.g. journalistic texts, scientific writing) and authors/speakers don't always present their evaluations in a straightforward way (e.g. some political texts). Take some time to explore some different kinds of texts (e.g. editorials, fiction, tweets, news articles, political speeches, texts from the corpus you built for the Corpus Building Project). Vader will tend to perform better with short texts, so make sure you try texts of different lengths.  \n",
    "**QUESTION:** How does Vader perform on different kinds of texts? What kinds of texts are challenging for a lexicon-based approach to sentiment analysis? What kinds of texts are not appropriate for sentiment analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''\n",
    "Put your text samples here.\n",
    "'''\n",
    "vs = analyzer.polarity_scores(example)\n",
    "print(str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **ACTIVITY:** Under the Readings section on this week's AKO|LEARN page is a link to a Hugging Face Spaces that allows you to test a pre-trained models for Sentiment Analysis. We have also linked to the relevant model the web app is using. Try out some of the sentences from the movie review example above. Try other texts you have tested in the lab today.  \n",
    "**QUESTION:** How do these models perform compared with Vader? What are some of the advantages and disadvantages of using pre-trained machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss what you have found with your neighbour. If you have time at the end of the lab you can work on your assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
